2021-04-28 11:01:50 [scrapy] ERROR: Error downloading <GET https:///robots.txt>: Empty domain
Traceback (most recent call last):
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/internet/defer.py", line 1443, in _inlineCallbacks
    result = current_context.run(result.throwExceptionIntoGenerator, g)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/python/failure.py", line 500, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/middleware.py", line 44, in process_request
    return (yield download_func(request=request, spider=spider))
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py", line 55, in mustbe_deferred
    result = f(*args, **kw)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/handlers/__init__.py", line 75, in download_request
    return handler.download_request(request, spider)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/handlers/http11.py", line 65, in download_request
    return agent.download_request(request)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/handlers/http11.py", line 335, in download_request
    d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/web/client.py", line 1753, in request
    endpoint = self._getEndpoint(parsedURI)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/web/client.py", line 1737, in _getEndpoint
    return self._endpointFactory.endpointForURI(uri)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/web/client.py", line 1608, in endpointForURI
    connectionCreator = self._policyForHTTPS.creatorForNetloc(
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/contextfactory.py", line 67, in creatorForNetloc
    return ScrapyClientTLSOptions(hostname.decode("ascii"), self.getContext(),
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/core/downloader/tls.py", line 42, in __init__
    super().__init__(hostname, ctx)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/internet/_sslverify.py", line 1130, in __init__
    self._hostnameBytes = _idnaBytes(hostname)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/twisted/internet/_idna.py", line 31, in _idnaBytes
    return idna.encode(text)
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/idna/core.py", line 357, in encode
    raise IDNAError('Empty domain')
idna.core.IDNAError: Empty domain
2021-04-28 11:01:50 [scrapy] ERROR: Spider error processing <GET https:///www.metacritic.com/game/playstation-2/grimgrimoire> (referer: None)
Traceback (most recent call last):
  File "/home/kardel/anaconda3/lib/python3.8/site-packages/scrapy/utils/defer.py", line 120, in iter_errback
    yield next(it)
  File "/home/kardel/SI507Final/SI507Final/spiders/game_detail_spider.py", line 50, in parse
    print("game.url", response.url)
AttributeError: 'Failure' object has no attribute 'url'
2021-04-28 11:01:50 [scrapy] INFO: Closing spider (finished)
2021-04-28 11:01:50 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/idna.core.IDNAError': 2,
 'downloader/request_bytes': 468,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'elapsed_time_seconds': 0.367745,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 4, 28, 16, 1, 50, 307877),
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'memusage/max': 57868288,
 'memusage/startup': 57868288,
 "robotstxt/exception_count/<class 'idna.core.IDNAError'>": 1,
 'robotstxt/request_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2021, 4, 28, 16, 1, 49, 940132)}
2021-04-28 11:01:50 [scrapy] INFO: Spider closed (finished)
